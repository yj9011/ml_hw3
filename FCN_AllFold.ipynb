{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36020302-2e98-4c81-9814-c36dd4fb380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if not hasattr(__builtins__, 'object'):\n",
    "    np.object = object\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "\n",
    "def load_image_mask_pairs(image_dir, mask_dir, img_size=(256, 256)):\n",
    "    mask_files = sorted(os.listdir(mask_dir))\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "\n",
    "    masks = []\n",
    "    images = []\n",
    "\n",
    "    for img_name, mask_name in zip(image_files, mask_files):\n",
    "        mask_path = os.path.join(mask_dir, mask_name)\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "\n",
    "        image = load_img(img_path, target_size=img_size)\n",
    "        image = img_to_array(image) / 255.0\n",
    "\n",
    "        mask = load_img(mask_path, target_size=img_size, color_mode=\"grayscale\")\n",
    "        mask = img_to_array(mask) / 255.0\n",
    "        mask = (mask > 0.5).astype(np.float32)\n",
    "\n",
    "        images.append(image)\n",
    "        masks.append(mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "base_path = os.path.join('C:', os.sep, 'Users', 'USER', 'python', 'pj3', 'ETT-v3', 'Fold1')\n",
    "\n",
    "train_images, train_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'train'),\n",
    "    os.path.join(base_path, 'trainannot')\n",
    ")\n",
    "\n",
    "val_images, val_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'val'),\n",
    "    os.path.join(base_path, 'valannot')\n",
    ")\n",
    "\n",
    "test_images, test_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'test'),\n",
    "    os.path.join(base_path, 'testannot')\n",
    ")\n",
    "\n",
    "print(len(train_images), len(train_masks))\n",
    "print(len(val_images), len(val_masks))\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_gen_args = dict(            \n",
    "    width_shift_range=0.05,     #左右隨機平移\n",
    "    rotation_range=10,   #輕微旋轉  \n",
    "    height_shift_range=0.05,    #上下隨機平移            \n",
    "    zoom_range=[0.9, 1.1],    #隨機裁切與縮放 \n",
    "    shear_range=0.05,       #剪切變形範圍  \n",
    "    horizontal_flip=True,    #水平翻轉          \n",
    "    brightness_range=[0.85, 1.15],   #亮度變化範圍\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def augment_dataset(images, masks, augment_times=2, batch_size=8):\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    image_gen = image_datagen.flow(images, batch_size=batch_size, seed=1)\n",
    "    mask_gen = mask_datagen.flow(masks, batch_size=batch_size, seed=1)\n",
    "\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "\n",
    "    steps = (len(images) * augment_times) // batch_size\n",
    "\n",
    "    for _ in range(steps):\n",
    "        img_batch = next(image_gen)\n",
    "        mask_batch = next(mask_gen)\n",
    "        augmented_images.extend(img_batch)\n",
    "        augmented_masks.extend(mask_batch)\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_masks)\n",
    "\n",
    "\n",
    "aug_train_images, aug_train_masks = augment_dataset(train_images, train_masks, augment_times=2)\n",
    "\n",
    "train_images = np.concatenate([train_images, aug_train_images])\n",
    "train_masks = np.concatenate([train_masks, aug_train_masks])\n",
    "\n",
    "print((train_images.shape),(test_images.shape))\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    union = tf.reduce_sum(tf.cast(y_true + y_pred > 0, tf.float32))\n",
    "    return intersection / (union + tf.keras.backend.epsilon())\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "\n",
    "def fcn_model(input_size=(256, 256, 3)):\n",
    "    inputs = Input(shape=input_size)\n",
    "\n",
    "    # Encoder (VGG-like)\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)  # 128x128\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)  # 64x64\n",
    "\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)  # 32x32\n",
    "\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)  # 16x16\n",
    "\n",
    "    conv5 = Conv2D(4096, (7, 7), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Dropout(0.5)(conv5)\n",
    "    conv5 = Conv2D(4096, (1, 1), activation='relu', padding='same')(conv5)\n",
    "    conv5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    score_fr = Conv2D(1, (1, 1), activation=None, padding='same')(conv5)\n",
    "    \n",
    "    # 多段上採樣 (從 16x16 → 256x256)\n",
    "    x = Conv2DTranspose(512, kernel_size=3, strides=2, padding='same')(score_fr)\n",
    "    x = Conv2DTranspose(256, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = Conv2DTranspose(64, kernel_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy', iou])\n",
    "    return model\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "Fold_number = 1\n",
    "\n",
    "model = fcn_model()\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    f'ETT_fcn_Fold{Fold_number}.keras',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history_model = model.fit(\n",
    "    train_images, train_masks,\n",
    "    batch_size=2,\n",
    "    epochs=30,\n",
    "    validation_data=(val_images, val_masks),\n",
    "    callbacks=[model_checkpoint, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80727a-65bf-4e40-aa77-47fdf1591864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, tensorflow as tf, cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "Fold_number = 1\n",
    "PIXELS_PER_CM = 72\n",
    "\n",
    "# ========================\n",
    "# 1. 模型預測與績效計算\n",
    "# ========================\n",
    "# 載入測試資料\n",
    "test_images, test_masks = load_image_mask_pairs(\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'test'),\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'testannot')\n",
    ")\n",
    "\n",
    "# 提前準備圖像檔案路徑（用於可視化）\n",
    "test_image_dir = f'ETT-v3/Fold{Fold_number}/test'\n",
    "test_image_filenames = sorted(os.listdir(test_image_dir))\n",
    "test_image_paths = [os.path.join(test_image_dir, name) for name in test_image_filenames]\n",
    "\n",
    "# 模型預測\n",
    "model = fcn_model()\n",
    "model.load_weights(f'ETT_fcn_Fold{Fold_number}.keras')\n",
    "results = model.predict(test_images, batch_size=8, verbose=1)\n",
    "results = (results > 0.5).astype(int)\n",
    "\n",
    "# 儲存預測遮罩\n",
    "def labelVisualize(num_class, color_dict, img):\n",
    "    img = img[:, :, 0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i, :] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "def saveResult(save_path, npyfile, flag_multi_class=False, num_class=2):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class, np.array([[0, 0, 0], [255, 255, 255]]), item) if flag_multi_class else item[:, :, 0]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img).convert('RGB')\n",
    "        img.save(os.path.join(save_path, f\"{i}_predict.jpeg\"), 'JPEG')\n",
    "\n",
    "saveResult(f'ETT/Fold{Fold_number}/predannot_fcn/', results)\n",
    "\n",
    "# 評估指標計算\n",
    "true_images = test_masks\n",
    "pred_images = results\n",
    "\n",
    "def get_lowest_y_coordinate(mask):\n",
    "    for idx in range(mask.shape[0] - 1, -1, -1):\n",
    "        if np.any(mask[idx]):\n",
    "            return idx\n",
    "    return 0\n",
    "\n",
    "def extract_lowest_y_coordinates(gt_list, pred_list):\n",
    "    gt_coords = [get_lowest_y_coordinate(gt) for gt in gt_list]\n",
    "    pred_coords = [get_lowest_y_coordinate(pd) for pd in pred_list]\n",
    "    return tf.constant(gt_coords, dtype=tf.float32), tf.constant(pred_coords, dtype=tf.float32)\n",
    "\n",
    "def compute_iou(gt_mask, pred_mask):\n",
    "    metric = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "    metric.update_state(gt_mask, pred_mask)\n",
    "    return metric.result().numpy()\n",
    "\n",
    "def mean_vertical_error(gt_list, pred_list):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    return tf.reduce_mean(tf.abs(predicted_y - actual_y)) / PIXELS_PER_CM\n",
    "\n",
    "def precision_within_threshold(gt_list, pred_list, threshold):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.abs(predicted_y - actual_y) <= threshold, tf.float32)) * 100.0\n",
    "    return acc.numpy()\n",
    "\n",
    "def precision_within_half_cm(gt, pd): return precision_within_threshold(gt, pd, 36)\n",
    "def precision_within_one_cm(gt, pd): return precision_within_threshold(gt, pd, 72)\n",
    "\n",
    "print(\"📊 Evaluation Results\")\n",
    "print(f\"IoU: {compute_iou(true_images, pred_images):.4f}\")\n",
    "print(f\"Avg Vertical Error (cm): {mean_vertical_error(true_images, pred_images).numpy():.4f}\")\n",
    "print(f\"Acc ≤ 0.5 cm: {precision_within_half_cm(true_images, pred_images):.2f}%\")\n",
    "print(f\"Acc ≤ 1.0 cm: {precision_within_one_cm(true_images, pred_images):.2f}%\")\n",
    "\n",
    "# ========================\n",
    "# 2. 圖像可視化（未標記端點）\n",
    "# ========================\n",
    "def preprocess_for_display(mask_array, cmap='plasma'):\n",
    "    plt.imshow(mask_array.squeeze(), cmap=cmap)\n",
    "    plt.axis('off')\n",
    "\n",
    "i = 0  # 預覽哪張圖\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"Ground Truth Mask\")\n",
    "preprocess_for_display(test_masks[i])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"Predicted Mask\")\n",
    "preprocess_for_display(results[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 3. 圖像可視化（已標記端點）\n",
    "# ========================\n",
    "def show_image_with_endpoint(mask_array, title, label=None, color='red'):\n",
    "    img = mask_array.squeeze()\n",
    "    y = get_lowest_y_coordinate(img)\n",
    "    plt.imshow(img, cmap='plasma')\n",
    "    if label:\n",
    "        plt.scatter(img.shape[1] // 2, y, c=color, s=30)\n",
    "        plt.text(img.shape[1] // 2 + 5, y, label, color=color,\n",
    "                 fontsize=12, fontweight='bold', va='center')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.title('Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "show_image_with_endpoint(test_masks[i], \"Ground Truth Mask with Endpoint\", label=\"G\", color='red')\n",
    "\n",
    "plt.subplot(133)\n",
    "show_image_with_endpoint(results[i], \"Predicted Mask with Endpoint\", label=\"Y\", color='yellow')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471c347-827f-4c08-92b0-4adf1954dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "del model\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "print(f\"🧹 Fold{Fold_number} 記憶體已釋放\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671dd6f7-232f-419b-9db6-2342edad19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ⬇ Fold2 的 Cell 開頭，清除上一輪模型記憶體 ==========\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# ========== ⬇ 載入 Fold2 的資料集 ==========\n",
    "Fold_number = 2  # <--- 改成你這次要跑的 Fold 編號\n",
    "\n",
    "base_path = os.path.join('C:', os.sep, 'Users', 'USER', 'python', 'pj3', 'ETT-v3', f'Fold{Fold_number}')\n",
    "train_images, train_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'train'),\n",
    "    os.path.join(base_path, 'trainannot')\n",
    ")\n",
    "val_images, val_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'val'),\n",
    "    os.path.join(base_path, 'valannot')\n",
    ")\n",
    "\n",
    "# 資料增強（如需跳過可註解）\n",
    "aug_train_images, aug_train_masks = augment_dataset(train_images, train_masks, augment_times=2)\n",
    "train_images = np.concatenate([train_images, aug_train_images])\n",
    "train_masks = np.concatenate([train_masks, aug_train_masks])\n",
    "\n",
    "print(f\"✅ Fold{Fold_number} loaded:\", train_images.shape, val_images.shape)\n",
    "\n",
    "# ========== ⬇ 建立與訓練模型 ==========\n",
    "model = fcn_model()\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    f'ETT_fcn_Fold{Fold_number}.keras',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history_model = model.fit(\n",
    "    train_images, train_masks,\n",
    "    batch_size=4,\n",
    "    epochs=10,\n",
    "    validation_data=(val_images, val_masks),\n",
    "    callbacks=[model_checkpoint, early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aae963-74e0-4330-957e-a663ebe13bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, tensorflow as tf, cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "Fold_number = 2\n",
    "PIXELS_PER_CM = 72\n",
    "\n",
    "# ========================\n",
    "# 1. 模型預測與績效計算\n",
    "# ========================\n",
    "# 載入測試資料\n",
    "test_images, test_masks = load_image_mask_pairs(\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'test'),\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'testannot')\n",
    ")\n",
    "\n",
    "# 提前準備圖像檔案路徑（用於可視化）\n",
    "test_image_dir = f'ETT-v3/Fold{Fold_number}/test'\n",
    "test_image_filenames = sorted(os.listdir(test_image_dir))\n",
    "test_image_paths = [os.path.join(test_image_dir, name) for name in test_image_filenames]\n",
    "\n",
    "# 模型預測\n",
    "model = fcn_model()\n",
    "model.load_weights(f'ETT_fcn_Fold{Fold_number}.keras')\n",
    "results = model.predict(test_images, batch_size=8, verbose=1)\n",
    "results = (results > 0.5).astype(int)\n",
    "\n",
    "# 儲存預測遮罩\n",
    "def labelVisualize(num_class, color_dict, img):\n",
    "    img = img[:, :, 0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i, :] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "def saveResult(save_path, npyfile, flag_multi_class=False, num_class=2):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class, np.array([[0, 0, 0], [255, 255, 255]]), item) if flag_multi_class else item[:, :, 0]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img).convert('RGB')\n",
    "        img.save(os.path.join(save_path, f\"{i}_predict.jpeg\"), 'JPEG')\n",
    "\n",
    "saveResult(f'ETT/Fold{Fold_number}/predannot_fcn/', results)\n",
    "\n",
    "# 評估指標計算\n",
    "true_images = test_masks\n",
    "pred_images = results\n",
    "\n",
    "def get_lowest_y_coordinate(mask):\n",
    "    for idx in range(mask.shape[0] - 1, -1, -1):\n",
    "        if np.any(mask[idx]):\n",
    "            return idx\n",
    "    return 0\n",
    "\n",
    "def extract_lowest_y_coordinates(gt_list, pred_list):\n",
    "    gt_coords = [get_lowest_y_coordinate(gt) for gt in gt_list]\n",
    "    pred_coords = [get_lowest_y_coordinate(pd) for pd in pred_list]\n",
    "    return tf.constant(gt_coords, dtype=tf.float32), tf.constant(pred_coords, dtype=tf.float32)\n",
    "\n",
    "def compute_iou(gt_mask, pred_mask):\n",
    "    metric = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "    metric.update_state(gt_mask, pred_mask)\n",
    "    return metric.result().numpy()\n",
    "\n",
    "def mean_vertical_error(gt_list, pred_list):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    return tf.reduce_mean(tf.abs(predicted_y - actual_y)) / PIXELS_PER_CM\n",
    "\n",
    "def precision_within_threshold(gt_list, pred_list, threshold):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.abs(predicted_y - actual_y) <= threshold, tf.float32)) * 100.0\n",
    "    return acc.numpy()\n",
    "\n",
    "def precision_within_half_cm(gt, pd): return precision_within_threshold(gt, pd, 36)\n",
    "def precision_within_one_cm(gt, pd): return precision_within_threshold(gt, pd, 72)\n",
    "\n",
    "print(\"📊 Evaluation Results\")\n",
    "print(f\"IoU: {compute_iou(true_images, pred_images):.4f}\")\n",
    "print(f\"Avg Vertical Error (cm): {mean_vertical_error(true_images, pred_images).numpy():.4f}\")\n",
    "print(f\"Acc ≤ 0.5 cm: {precision_within_half_cm(true_images, pred_images):.2f}%\")\n",
    "print(f\"Acc ≤ 1.0 cm: {precision_within_one_cm(true_images, pred_images):.2f}%\")\n",
    "\n",
    "# ========================\n",
    "# 2. 圖像可視化（未標記端點）\n",
    "# ========================\n",
    "def preprocess_for_display(mask_array, cmap='plasma'):\n",
    "    plt.imshow(mask_array.squeeze(), cmap=cmap)\n",
    "    plt.axis('off')\n",
    "\n",
    "i = 0  # 預覽哪張圖\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"Ground Truth Mask\")\n",
    "preprocess_for_display(test_masks[i])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"Predicted Mask\")\n",
    "preprocess_for_display(results[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 3. 圖像可視化（已標記端點）\n",
    "# ========================\n",
    "def show_image_with_endpoint(mask_array, title, label=None, color='red'):\n",
    "    img = mask_array.squeeze()\n",
    "    y = get_lowest_y_coordinate(img)\n",
    "    plt.imshow(img, cmap='plasma')\n",
    "    if label:\n",
    "        plt.scatter(img.shape[1] // 2, y, c=color, s=30)\n",
    "        plt.text(img.shape[1] // 2 + 5, y, label, color=color,\n",
    "                 fontsize=12, fontweight='bold', va='center')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.title('Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "show_image_with_endpoint(test_masks[i], \"Ground Truth Mask with Endpoint\", label=\"G\", color='red')\n",
    "\n",
    "plt.subplot(133)\n",
    "show_image_with_endpoint(results[i], \"Predicted Mask with Endpoint\", label=\"Y\", color='yellow')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd21343-b350-4d7e-8ff8-07f88fa027a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "del model\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "print(f\"🧹 Fold{Fold_number} 記憶體已釋放\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327836e-2d27-43ab-a6d2-833f723b3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# ========== 載入資料集 ==========\n",
    "Fold_number = 3  # <--- 改成你這次要跑的 Fold 編號\n",
    "\n",
    "base_path = os.path.join('C:', os.sep, 'Users', 'USER', 'python', 'pj3', 'ETT-v3', f'Fold{Fold_number}')\n",
    "train_images, train_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'train'),\n",
    "    os.path.join(base_path, 'trainannot')\n",
    ")\n",
    "val_images, val_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'val'),\n",
    "    os.path.join(base_path, 'valannot')\n",
    ")\n",
    "\n",
    "# 資料增強\n",
    "aug_train_images, aug_train_masks = augment_dataset(train_images, train_masks, augment_times=2)\n",
    "train_images = np.concatenate([train_images, aug_train_images])\n",
    "train_masks = np.concatenate([train_masks, aug_train_masks])\n",
    "\n",
    "print(f\"✅ Fold{Fold_number} loaded:\", train_images.shape, val_images.shape)\n",
    "\n",
    "# ========== ⬇ 建立與訓練模型 ==========\n",
    "model = fcn_model()\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    f'ETT_fcn_Fold{Fold_number}.keras',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history_model = model.fit(\n",
    "    train_images, train_masks,\n",
    "    batch_size=4,\n",
    "    epochs=10,\n",
    "    validation_data=(val_images, val_masks),\n",
    "    callbacks=[model_checkpoint, early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110dee5-1c96-4747-a713-d81ca988e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, tensorflow as tf, cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "Fold_number = 3\n",
    "PIXELS_PER_CM = 72\n",
    "\n",
    "# ========================\n",
    "# 1. 模型預測與績效計算\n",
    "# ========================\n",
    "# 載入測試資料\n",
    "test_images, test_masks = load_image_mask_pairs(\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'test'),\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'testannot')\n",
    ")\n",
    "\n",
    "# 提前準備圖像檔案路徑（用於可視化）\n",
    "test_image_dir = f'ETT-v3/Fold{Fold_number}/test'\n",
    "test_image_filenames = sorted(os.listdir(test_image_dir))\n",
    "test_image_paths = [os.path.join(test_image_dir, name) for name in test_image_filenames]\n",
    "\n",
    "# 模型預測\n",
    "model = fcn_model()\n",
    "model.load_weights(f'ETT_fcn_Fold{Fold_number}.keras')\n",
    "results = model.predict(test_images, batch_size=8, verbose=1)\n",
    "results = (results > 0.5).astype(int)\n",
    "\n",
    "# 儲存預測遮罩\n",
    "def labelVisualize(num_class, color_dict, img):\n",
    "    img = img[:, :, 0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i, :] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "def saveResult(save_path, npyfile, flag_multi_class=False, num_class=2):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class, np.array([[0, 0, 0], [255, 255, 255]]), item) if flag_multi_class else item[:, :, 0]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img).convert('RGB')\n",
    "        img.save(os.path.join(save_path, f\"{i}_predict.jpeg\"), 'JPEG')\n",
    "\n",
    "saveResult(f'ETT/Fold{Fold_number}/predannot_fcn/', results)\n",
    "\n",
    "# 評估指標計算\n",
    "true_images = test_masks\n",
    "pred_images = results\n",
    "\n",
    "def get_lowest_y_coordinate(mask):\n",
    "    for idx in range(mask.shape[0] - 1, -1, -1):\n",
    "        if np.any(mask[idx]):\n",
    "            return idx\n",
    "    return 0\n",
    "\n",
    "def extract_lowest_y_coordinates(gt_list, pred_list):\n",
    "    gt_coords = [get_lowest_y_coordinate(gt) for gt in gt_list]\n",
    "    pred_coords = [get_lowest_y_coordinate(pd) for pd in pred_list]\n",
    "    return tf.constant(gt_coords, dtype=tf.float32), tf.constant(pred_coords, dtype=tf.float32)\n",
    "\n",
    "def compute_iou(gt_mask, pred_mask):\n",
    "    metric = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "    metric.update_state(gt_mask, pred_mask)\n",
    "    return metric.result().numpy()\n",
    "\n",
    "def mean_vertical_error(gt_list, pred_list):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    return tf.reduce_mean(tf.abs(predicted_y - actual_y)) / PIXELS_PER_CM\n",
    "\n",
    "def precision_within_threshold(gt_list, pred_list, threshold):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.abs(predicted_y - actual_y) <= threshold, tf.float32)) * 100.0\n",
    "    return acc.numpy()\n",
    "\n",
    "def precision_within_half_cm(gt, pd): return precision_within_threshold(gt, pd, 36)\n",
    "def precision_within_one_cm(gt, pd): return precision_within_threshold(gt, pd, 72)\n",
    "\n",
    "print(\"📊 Evaluation Results\")\n",
    "print(f\"IoU: {compute_iou(true_images, pred_images):.4f}\")\n",
    "print(f\"Avg Vertical Error (cm): {mean_vertical_error(true_images, pred_images).numpy():.4f}\")\n",
    "print(f\"Acc ≤ 0.5 cm: {precision_within_half_cm(true_images, pred_images):.2f}%\")\n",
    "print(f\"Acc ≤ 1.0 cm: {precision_within_one_cm(true_images, pred_images):.2f}%\")\n",
    "\n",
    "# ========================\n",
    "# 2. 圖像可視化（未標記端點）\n",
    "# ========================\n",
    "def preprocess_for_display(mask_array, cmap='plasma'):\n",
    "    plt.imshow(mask_array.squeeze(), cmap=cmap)\n",
    "    plt.axis('off')\n",
    "\n",
    "i = 0  # 預覽哪張圖\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"Ground Truth Mask\")\n",
    "preprocess_for_display(test_masks[i])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"Predicted Mask\")\n",
    "preprocess_for_display(results[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 3. 圖像可視化（已標記端點）\n",
    "# ========================\n",
    "def show_image_with_endpoint(mask_array, title, label=None, color='red'):\n",
    "    img = mask_array.squeeze()\n",
    "    y = get_lowest_y_coordinate(img)\n",
    "    plt.imshow(img, cmap='plasma')\n",
    "    if label:\n",
    "        plt.scatter(img.shape[1] // 2, y, c=color, s=30)\n",
    "        plt.text(img.shape[1] // 2 + 5, y, label, color=color,\n",
    "                 fontsize=12, fontweight='bold', va='center')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.title('Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "show_image_with_endpoint(test_masks[i], \"Ground Truth Mask with Endpoint\", label=\"G\", color='red')\n",
    "\n",
    "plt.subplot(133)\n",
    "show_image_with_endpoint(results[i], \"Predicted Mask with Endpoint\", label=\"Y\", color='yellow')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f67d9-6d98-4842-adfd-b417c79e5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "del model\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "print(f\"🧹 Fold{Fold_number} 記憶體已釋放\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65960e27-a771-403f-841e-d67d058f29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# ========== 載入資料集 ==========\n",
    "Fold_number = 4  # <--- 改成你這次要跑的 Fold 編號\n",
    "\n",
    "base_path = os.path.join('C:', os.sep, 'Users', 'USER', 'python', 'pj3', 'ETT-v3', f'Fold{Fold_number}')\n",
    "train_images, train_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'train'),\n",
    "    os.path.join(base_path, 'trainannot')\n",
    ")\n",
    "val_images, val_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'val'),\n",
    "    os.path.join(base_path, 'valannot')\n",
    ")\n",
    "\n",
    "# 資料增強\n",
    "aug_train_images, aug_train_masks = augment_dataset(train_images, train_masks, augment_times=2)\n",
    "train_images = np.concatenate([train_images, aug_train_images])\n",
    "train_masks = np.concatenate([train_masks, aug_train_masks])\n",
    "\n",
    "print(f\"✅ Fold{Fold_number} loaded:\", train_images.shape, val_images.shape)\n",
    "\n",
    "# ========== ⬇ 建立與訓練模型 ==========\n",
    "model = fcn_model()\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    f'ETT_fcn_Fold{Fold_number}.keras',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history_model = model.fit(\n",
    "    train_images, train_masks,\n",
    "    batch_size=4,\n",
    "    epochs=10,\n",
    "    validation_data=(val_images, val_masks),\n",
    "    callbacks=[model_checkpoint, early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17be6e-ccd6-41e5-af51-3853c4f3d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, tensorflow as tf, cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "Fold_number = 4\n",
    "PIXELS_PER_CM = 72\n",
    "\n",
    "# ========================\n",
    "# 1. 模型預測與績效計算\n",
    "# ========================\n",
    "# 載入測試資料\n",
    "test_images, test_masks = load_image_mask_pairs(\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'test'),\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'testannot')\n",
    ")\n",
    "\n",
    "# 提前準備圖像檔案路徑（用於可視化）\n",
    "test_image_dir = f'ETT-v3/Fold{Fold_number}/test'\n",
    "test_image_filenames = sorted(os.listdir(test_image_dir))\n",
    "test_image_paths = [os.path.join(test_image_dir, name) for name in test_image_filenames]\n",
    "\n",
    "# 模型預測\n",
    "model = fcn_model()\n",
    "model.load_weights(f'ETT_fcn_Fold{Fold_number}.keras')\n",
    "results = model.predict(test_images, batch_size=8, verbose=1)\n",
    "results = (results > 0.5).astype(int)\n",
    "\n",
    "# 儲存預測遮罩\n",
    "def labelVisualize(num_class, color_dict, img):\n",
    "    img = img[:, :, 0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i, :] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "def saveResult(save_path, npyfile, flag_multi_class=False, num_class=2):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class, np.array([[0, 0, 0], [255, 255, 255]]), item) if flag_multi_class else item[:, :, 0]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img).convert('RGB')\n",
    "        img.save(os.path.join(save_path, f\"{i}_predict.jpeg\"), 'JPEG')\n",
    "\n",
    "saveResult(f'ETT/Fold{Fold_number}/predannot_fcn/', results)\n",
    "\n",
    "# 評估指標計算\n",
    "true_images = test_masks\n",
    "pred_images = results\n",
    "\n",
    "def get_lowest_y_coordinate(mask):\n",
    "    for idx in range(mask.shape[0] - 1, -1, -1):\n",
    "        if np.any(mask[idx]):\n",
    "            return idx\n",
    "    return 0\n",
    "\n",
    "def extract_lowest_y_coordinates(gt_list, pred_list):\n",
    "    gt_coords = [get_lowest_y_coordinate(gt) for gt in gt_list]\n",
    "    pred_coords = [get_lowest_y_coordinate(pd) for pd in pred_list]\n",
    "    return tf.constant(gt_coords, dtype=tf.float32), tf.constant(pred_coords, dtype=tf.float32)\n",
    "\n",
    "def compute_iou(gt_mask, pred_mask):\n",
    "    metric = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "    metric.update_state(gt_mask, pred_mask)\n",
    "    return metric.result().numpy()\n",
    "\n",
    "def mean_vertical_error(gt_list, pred_list):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    return tf.reduce_mean(tf.abs(predicted_y - actual_y)) / PIXELS_PER_CM\n",
    "\n",
    "def precision_within_threshold(gt_list, pred_list, threshold):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.abs(predicted_y - actual_y) <= threshold, tf.float32)) * 100.0\n",
    "    return acc.numpy()\n",
    "\n",
    "def precision_within_half_cm(gt, pd): return precision_within_threshold(gt, pd, 36)\n",
    "def precision_within_one_cm(gt, pd): return precision_within_threshold(gt, pd, 72)\n",
    "\n",
    "print(\"📊 Evaluation Results\")\n",
    "print(f\"IoU: {compute_iou(true_images, pred_images):.4f}\")\n",
    "print(f\"Avg Vertical Error (cm): {mean_vertical_error(true_images, pred_images).numpy():.4f}\")\n",
    "print(f\"Acc ≤ 0.5 cm: {precision_within_half_cm(true_images, pred_images):.2f}%\")\n",
    "print(f\"Acc ≤ 1.0 cm: {precision_within_one_cm(true_images, pred_images):.2f}%\")\n",
    "\n",
    "# ========================\n",
    "# 2. 圖像可視化（未標記端點）\n",
    "# ========================\n",
    "def preprocess_for_display(mask_array, cmap='plasma'):\n",
    "    plt.imshow(mask_array.squeeze(), cmap=cmap)\n",
    "    plt.axis('off')\n",
    "\n",
    "i = 0  # 預覽哪張圖\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"Ground Truth Mask\")\n",
    "preprocess_for_display(test_masks[i])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"Predicted Mask\")\n",
    "preprocess_for_display(results[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 3. 圖像可視化（已標記端點）\n",
    "# ========================\n",
    "def show_image_with_endpoint(mask_array, title, label=None, color='red'):\n",
    "    img = mask_array.squeeze()\n",
    "    y = get_lowest_y_coordinate(img)\n",
    "    plt.imshow(img, cmap='plasma')\n",
    "    if label:\n",
    "        plt.scatter(img.shape[1] // 2, y, c=color, s=30)\n",
    "        plt.text(img.shape[1] // 2 + 5, y, label, color=color,\n",
    "                 fontsize=12, fontweight='bold', va='center')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.title('Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "show_image_with_endpoint(test_masks[i], \"Ground Truth Mask with Endpoint\", label=\"G\", color='red')\n",
    "\n",
    "plt.subplot(133)\n",
    "show_image_with_endpoint(results[i], \"Predicted Mask with Endpoint\", label=\"Y\", color='yellow')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44407f7-76fc-4873-84f9-2c17e1e7299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "del model\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "print(f\"🧹 Fold{Fold_number} 記憶體已釋放\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c10b4-5b51-4ed4-9b30-d0b9c55a8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# ========== 載入資料集 ==========\n",
    "Fold_number = 5  # <--- 改成你這次要跑的 Fold 編號\n",
    "\n",
    "base_path = os.path.join('C:', os.sep, 'Users', 'USER', 'python', 'pj3', 'ETT-v3', f'Fold{Fold_number}')\n",
    "train_images, train_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'train'),\n",
    "    os.path.join(base_path, 'trainannot')\n",
    ")\n",
    "val_images, val_masks = load_image_mask_pairs(\n",
    "    os.path.join(base_path, 'val'),\n",
    "    os.path.join(base_path, 'valannot')\n",
    ")\n",
    "\n",
    "# 資料增強\n",
    "aug_train_images, aug_train_masks = augment_dataset(train_images, train_masks, augment_times=2)\n",
    "train_images = np.concatenate([train_images, aug_train_images])\n",
    "train_masks = np.concatenate([train_masks, aug_train_masks])\n",
    "\n",
    "print(f\"✅ Fold{Fold_number} loaded:\", train_images.shape, val_images.shape)\n",
    "\n",
    "# ========== ⬇ 建立與訓練模型 ==========\n",
    "model = fcn_model()\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    f'ETT_fcn_Fold{Fold_number}.keras',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history_model = model.fit(\n",
    "    train_images, train_masks,\n",
    "    batch_size=4,\n",
    "    epochs=10,\n",
    "    validation_data=(val_images, val_masks),\n",
    "    callbacks=[model_checkpoint, early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47224138-f1f0-419a-8642-4666e6a1f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, tensorflow as tf, cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "Fold_number = 5\n",
    "PIXELS_PER_CM = 72\n",
    "\n",
    "# ========================\n",
    "# 1. 模型預測與績效計算\n",
    "# ========================\n",
    "# 載入測試資料\n",
    "test_images, test_masks = load_image_mask_pairs(\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'test'),\n",
    "    os.path.join(f'ETT-v3/Fold{Fold_number}', 'testannot')\n",
    ")\n",
    "\n",
    "# 提前準備圖像檔案路徑（用於可視化）\n",
    "test_image_dir = f'ETT-v3/Fold{Fold_number}/test'\n",
    "test_image_filenames = sorted(os.listdir(test_image_dir))\n",
    "test_image_paths = [os.path.join(test_image_dir, name) for name in test_image_filenames]\n",
    "\n",
    "# 模型預測\n",
    "model = fcn_model()\n",
    "model.load_weights(f'ETT_fcn_Fold{Fold_number}.keras')\n",
    "results = model.predict(test_images, batch_size=8, verbose=1)\n",
    "results = (results > 0.5).astype(int)\n",
    "\n",
    "# 儲存預測遮罩\n",
    "def labelVisualize(num_class, color_dict, img):\n",
    "    img = img[:, :, 0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i, :] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "def saveResult(save_path, npyfile, flag_multi_class=False, num_class=2):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class, np.array([[0, 0, 0], [255, 255, 255]]), item) if flag_multi_class else item[:, :, 0]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img).convert('RGB')\n",
    "        img.save(os.path.join(save_path, f\"{i}_predict.jpeg\"), 'JPEG')\n",
    "\n",
    "saveResult(f'ETT/Fold{Fold_number}/predannot_fcn/', results)\n",
    "\n",
    "# 評估指標計算\n",
    "true_images = test_masks\n",
    "pred_images = results\n",
    "\n",
    "def get_lowest_y_coordinate(mask):\n",
    "    for idx in range(mask.shape[0] - 1, -1, -1):\n",
    "        if np.any(mask[idx]):\n",
    "            return idx\n",
    "    return 0\n",
    "\n",
    "def extract_lowest_y_coordinates(gt_list, pred_list):\n",
    "    gt_coords = [get_lowest_y_coordinate(gt) for gt in gt_list]\n",
    "    pred_coords = [get_lowest_y_coordinate(pd) for pd in pred_list]\n",
    "    return tf.constant(gt_coords, dtype=tf.float32), tf.constant(pred_coords, dtype=tf.float32)\n",
    "\n",
    "def compute_iou(gt_mask, pred_mask):\n",
    "    metric = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "    metric.update_state(gt_mask, pred_mask)\n",
    "    return metric.result().numpy()\n",
    "\n",
    "def mean_vertical_error(gt_list, pred_list):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    return tf.reduce_mean(tf.abs(predicted_y - actual_y)) / PIXELS_PER_CM\n",
    "\n",
    "def precision_within_threshold(gt_list, pred_list, threshold):\n",
    "    actual_y, predicted_y = extract_lowest_y_coordinates(gt_list, pred_list)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.abs(predicted_y - actual_y) <= threshold, tf.float32)) * 100.0\n",
    "    return acc.numpy()\n",
    "\n",
    "def precision_within_half_cm(gt, pd): return precision_within_threshold(gt, pd, 36)\n",
    "def precision_within_one_cm(gt, pd): return precision_within_threshold(gt, pd, 72)\n",
    "\n",
    "print(\"📊 Evaluation Results\")\n",
    "print(f\"IoU: {compute_iou(true_images, pred_images):.4f}\")\n",
    "print(f\"Avg Vertical Error (cm): {mean_vertical_error(true_images, pred_images).numpy():.4f}\")\n",
    "print(f\"Acc ≤ 0.5 cm: {precision_within_half_cm(true_images, pred_images):.2f}%\")\n",
    "print(f\"Acc ≤ 1.0 cm: {precision_within_one_cm(true_images, pred_images):.2f}%\")\n",
    "\n",
    "# ========================\n",
    "# 2. 圖像可視化（未標記端點）\n",
    "# ========================\n",
    "def preprocess_for_display(mask_array, cmap='plasma'):\n",
    "    plt.imshow(mask_array.squeeze(), cmap=cmap)\n",
    "    plt.axis('off')\n",
    "\n",
    "i = 0  # 預覽哪張圖\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"Ground Truth Mask\")\n",
    "preprocess_for_display(test_masks[i])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"Predicted Mask\")\n",
    "preprocess_for_display(results[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 3. 圖像可視化（已標記端點）\n",
    "# ========================\n",
    "def show_image_with_endpoint(mask_array, title, label=None, color='red'):\n",
    "    img = mask_array.squeeze()\n",
    "    y = get_lowest_y_coordinate(img)\n",
    "    plt.imshow(img, cmap='plasma')\n",
    "    if label:\n",
    "        plt.scatter(img.shape[1] // 2, y, c=color, s=30)\n",
    "        plt.text(img.shape[1] // 2 + 5, y, label, color=color,\n",
    "                 fontsize=12, fontweight='bold', va='center')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(9, 2.5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(Image.open(test_image_paths[i]), cmap='gray')\n",
    "plt.title('Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "show_image_with_endpoint(test_masks[i], \"Ground Truth Mask with Endpoint\", label=\"G\", color='red')\n",
    "\n",
    "plt.subplot(133)\n",
    "show_image_with_endpoint(results[i], \"Predicted Mask with Endpoint\", label=\"Y\", color='yellow')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1edbb4-1398-40fb-a4da-4025e4ce0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "del model\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "print(f\"🧹 Fold{Fold_number} 記憶體已釋放\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
